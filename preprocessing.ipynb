{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf3e53611b04fc59a96d730febdc768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def preprocess_image_for_dense_breast(\n",
    "    image_path, \n",
    "    output_size=(1024, 1024),\n",
    "    normalize_min=-1024,   \n",
    "    normalize_max=3071,    \n",
    "    target_mean=0,        \n",
    "    target_std=1          \n",
    "):\n",
    "\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not load image {image_path}\")\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    img_clahe = clahe.apply(img)\n",
    "\n",
    "    img_float = img_clahe.astype(np.float32)\n",
    "    img_normalized = np.clip(img_float, normalize_min, normalize_max)\n",
    "\n",
    "    img_normalized = (img_normalized - np.mean(img_normalized)) / (np.std(img_normalized) + 1e-7)\n",
    "\n",
    "    if target_mean != 0 or target_std != 1:\n",
    "        img_normalized = img_normalized * target_std + target_mean\n",
    "\n",
    "    img_uint8 = ((img_normalized - img_normalized.min()) * 255 / \n",
    "                 (img_normalized.max() - img_normalized.min()))\n",
    "    img_uint8 = img_uint8.astype(np.uint8)\n",
    "    \n",
    "    img_resized = cv2.resize(img_uint8, output_size, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    return img_resized\n",
    "\n",
    "\n",
    "\n",
    "def process_all_images(data_dir, output_dir, output_size=(1024, 1024), show_samples=True):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    image_files = []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(('.jpg', '.jpeg', '.png', '.tiff')):\n",
    "                image_files.append((root, file))\n",
    "\n",
    "    for root, file in tqdm(image_files, desc=\"Processing images\"):\n",
    "        image_path = os.path.join(root, file)\n",
    "        \n",
    "        try:\n",
    "            if show_samples:\n",
    "                original = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            processed_image = preprocess_image_for_dense_breast(image_path, output_size)\n",
    "\n",
    "            output_path = os.path.join(output_dir, os.path.relpath(image_path, data_dir))\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            cv2.imwrite(output_path, processed_image)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {image_path}: {e}\")\n",
    "\n",
    "process_all_images(\n",
    "    r'D:\\RESEARCH\\MV-DEFEAT\\datasets\\VinDr-mammo\\images',\n",
    "    r'D:\\RESEARCH\\MV-DEFEAT\\datasets\\VinDr-mammo\\processed_images',\n",
    "    output_size=(512, 512)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bbc8d36a519493f9359274f2a49b326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images:   0%|          | 0/2744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def crop_breast_region(img):\n",
    "    _, binary = cv2.threshold(img, 5, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        \n",
    "        cropped = img[y:y+h, x:x+w]\n",
    "        return cropped\n",
    "    \n",
    "    return img\n",
    "\n",
    "def preprocess_image_for_dense_breast(\n",
    "    image_path, \n",
    "    output_size=(1024, 1024),\n",
    "    normalize_min=-1024,   \n",
    "    normalize_max=3071,     \n",
    "    target_mean=0,         \n",
    "    target_std=1          \n",
    "):\n",
    "\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not load image {image_path}\")\n",
    "\n",
    "    img = crop_breast_region(img)\n",
    "\n",
    "    img_float = img.astype(np.float32)\n",
    "    img_normalized = np.clip(img_float, normalize_min, normalize_max)\n",
    "\n",
    "    img_normalized = (img_normalized - np.mean(img_normalized)) / (np.std(img_normalized) + 1e-7)\n",
    "\n",
    "    if target_mean != 0 or target_std != 1:\n",
    "        img_normalized = img_normalized * target_std + target_mean\n",
    "\n",
    "    img_uint8 = ((img_normalized - img_normalized.min()) * 255 / \n",
    "                 (img_normalized.max() - img_normalized.min()))\n",
    "    img_uint8 = img_uint8.astype(np.uint8)\n",
    "\n",
    "    img_resized = cv2.resize(img_uint8, output_size, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    return img_resized\n",
    "\n",
    "def process_all_images(data_dir, output_dir, output_size=(1024, 1024), show_samples=True):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    image_files = []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(('.jpg', '.jpeg', '.png', '.tiff')):\n",
    "                image_files.append((root, file))\n",
    "\n",
    "    for root, file in tqdm(image_files, desc=\"Processing images\"):\n",
    "        image_path = os.path.join(root, file)\n",
    "        \n",
    "        try:\n",
    "            if show_samples:\n",
    "                original = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            processed_image = preprocess_image_for_dense_breast(image_path, output_size)\n",
    "            \n",
    "            output_path = os.path.join(output_dir, os.path.relpath(image_path, data_dir))\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            cv2.imwrite(output_path, processed_image)\n",
    "            \n",
    "            if show_samples:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.imshow(original, cmap='gray')\n",
    "                plt.title('Original Image')\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.imshow(processed_image, cmap='gray')\n",
    "                plt.title('Processed Image')\n",
    "                plt.show()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {image_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_images(\n",
    "        r'D:\\RESEARCH\\MV-DEFEAT\\datasets\\ThongNhat\\images',\n",
    "        r'D:\\RESEARCH\\MV-DEFEAT\\datasets\\ThongNhat\\processed_images',\n",
    "        output_size=(512, 512),\n",
    "        show_samples=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 distributions:\n",
      "\n",
      "Train set distribution:\n",
      "Label\n",
      "C    0.433453\n",
      "D    0.338129\n",
      "B    0.196043\n",
      "A    0.032374\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation set distribution:\n",
      "Label\n",
      "C    0.435714\n",
      "D    0.335714\n",
      "B    0.192857\n",
      "A    0.035714\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fold 1 distributions:\n",
      "\n",
      "Train set distribution:\n",
      "Label\n",
      "C    0.432675\n",
      "D    0.337522\n",
      "B    0.195691\n",
      "A    0.034111\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation set distribution:\n",
      "Label\n",
      "C    0.438849\n",
      "D    0.338129\n",
      "B    0.194245\n",
      "A    0.028777\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fold 2 distributions:\n",
      "\n",
      "Train set distribution:\n",
      "Label\n",
      "C    0.434470\n",
      "D    0.337522\n",
      "B    0.193896\n",
      "A    0.034111\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation set distribution:\n",
      "Label\n",
      "C    0.431655\n",
      "D    0.338129\n",
      "B    0.201439\n",
      "A    0.028777\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fold 3 distributions:\n",
      "\n",
      "Train set distribution:\n",
      "Label\n",
      "C    0.434470\n",
      "D    0.337522\n",
      "B    0.195691\n",
      "A    0.032316\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation set distribution:\n",
      "Label\n",
      "C    0.431655\n",
      "D    0.338129\n",
      "B    0.194245\n",
      "A    0.035971\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fold 4 distributions:\n",
      "\n",
      "Train set distribution:\n",
      "Label\n",
      "C    0.434470\n",
      "D    0.337522\n",
      "B    0.195691\n",
      "A    0.032316\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation set distribution:\n",
      "Label\n",
      "C    0.431655\n",
      "D    0.338129\n",
      "B    0.194245\n",
      "A    0.035971\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def create_kfold_splits(csv_path, n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Create k-fold splits for the dataset while maintaining class distribution\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # mapping\n",
    "    label_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "    df['Label_num'] = df['Label'].map(label_map)\n",
    "    \n",
    "    # define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # add fold\n",
    "    df['fold'] = -1\n",
    "    \n",
    "    # split on label for class distribution\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['Label_num'])):\n",
    "        df.loc[val_idx, 'fold'] = fold\n",
    "    \n",
    "    # save fold\n",
    "    base_path = os.path.dirname(csv_path)\n",
    "    filename = os.path.basename(csv_path).split('.')[0]\n",
    "    \n",
    "    for fold in range(n_splits):\n",
    "        # train data for current fold\n",
    "        train_df = df[df['fold'] != fold].copy()\n",
    "        # validation data for current fold\n",
    "        val_df = df[df['fold'] == fold].copy()\n",
    "        \n",
    "        # save files as patient, label\n",
    "        train_df[['Patient', 'Label']].to_csv(\n",
    "            os.path.join(base_path, f'{filename}_fold{fold}_train.csv'), \n",
    "            index=False\n",
    "        )\n",
    "        val_df[['Patient', 'Label']].to_csv(\n",
    "            os.path.join(base_path, f'{filename}_fold{fold}_val.csv'), \n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "        # print class distribution for each fold\n",
    "        print(f\"\\nFold {fold} distributions:\")\n",
    "        print(\"\\nTrain set distribution:\")\n",
    "        print(train_df['Label'].value_counts(normalize=True))\n",
    "        print(\"\\nValidation set distribution:\")\n",
    "        print(val_df['Label'].value_counts(normalize=True))\n",
    "    \n",
    "    # save origin file with fold\n",
    "    df.to_csv(os.path.join(base_path, f'{filename}_with_folds.csv'), index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = 'datasets/ThongNhat/ThongNhat_labels.csv'\n",
    "    create_kfold_splits(csv_path, n_splits=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20000 records\n",
      "Saved to: D:\\RESEARCH\\MV-DEFEAT\\datasets\\VinDr-mammo\\mammogram_metadata.csv\n",
      "\n",
      "Value counts for breast density:\n",
      "breast_density\n",
      "2    15292\n",
      "3     2700\n",
      "1     1908\n",
      "0      100\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for view position:\n",
      "view_position\n",
      "CC     10001\n",
      "MLO     9999\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for laterality:\n",
      "laterality\n",
      "L    10000\n",
      "R    10000\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hieup\\AppData\\Local\\Temp\\ipykernel_20960\\2842166915.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['breast_density'] = df_selected['breast_density'].str.replace('DENSITY ', '')\n",
      "C:\\Users\\hieup\\AppData\\Local\\Temp\\ipykernel_20960\\2842166915.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['breast_density'] = df_selected['breast_density'].map(density_mapping)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def extract_mammogram_info(input_csv_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Extract specific columns from mammogram annotations file and save to new CSV.\n",
    "    \n",
    "    Args:\n",
    "        input_csv_path (str): Path to input CSV file\n",
    "        output_csv_path (str): Path to save output CSV file\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    selected_columns = [\n",
    "        'study_id',\n",
    "        'image_id',\n",
    "        'laterality',\n",
    "        'view_position',\n",
    "        'breast_density'\n",
    "    ]\n",
    "    \n",
    "    df_selected = df[selected_columns]\n",
    "    \n",
    "    df_selected['breast_density'] = df_selected['breast_density'].str.replace('DENSITY ', '')\n",
    "    density_mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "    df_selected['breast_density'] = df_selected['breast_density'].map(density_mapping)\n",
    "    \n",
    "    \n",
    "    df_selected.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Processed {len(df_selected)} records\")\n",
    "    print(f\"Saved to: {output_csv_path}\")\n",
    "\n",
    "    print(\"\\nValue counts for breast density:\")\n",
    "    print(df_selected['breast_density'].value_counts())\n",
    "    print(\"\\nValue counts for view position:\")\n",
    "    print(df_selected['view_position'].value_counts())\n",
    "    print(\"\\nValue counts for laterality:\")\n",
    "    print(df_selected['laterality'].value_counts())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = r'D:\\RESEARCH\\MV-DEFEAT\\datasets\\VinDr-mammo\\breast-level_annotations.csv'\n",
    "    output_path = r'D:\\RESEARCH\\MV-DEFEAT\\datasets\\VinDr-mammo\\mammogram_metadata.csv'\n",
    "    \n",
    "    extract_mammogram_info(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original density distribution:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'breast_density'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\APP\\anaconda\\envs\\torch_env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'breast_density'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 82\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     81\u001b[0m     csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRESEARCH\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMV-DEFEAT\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mThongNhat\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mThongNhat_labels.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 82\u001b[0m     train_df, val_df, test_df \u001b[38;5;241m=\u001b[39m \u001b[43msplit_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m, in \u001b[0;36msplit_dataset\u001b[1;34m(csv_path, train_ratio, val_ratio, test_ratio, random_state)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Kiểm tra phân phối breast_density ban đầu\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal density distribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m original_dist \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbreast_density\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalue_counts(normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(original_dist)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Lấy unique studies và density của chúng\u001b[39;00m\n",
      "File \u001b[1;32md:\\APP\\anaconda\\envs\\torch_env\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32md:\\APP\\anaconda\\envs\\torch_env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'breast_density'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from collections import Counter\n",
    "\n",
    "def split_dataset(csv_path, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2, random_state=42):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(\"Original density distribution:\")\n",
    "    original_dist = df['breast_density'].value_counts(normalize=True)\n",
    "    print(original_dist)\n",
    "    \n",
    "    study_density = df.groupby('study_id')['breast_density'].first().reset_index()\n",
    "    \n",
    "    train_splitter = GroupShuffleSplit(n_splits=1, train_size=train_ratio, random_state=random_state)\n",
    "    train_idx, temp_idx = next(train_splitter.split(study_density, groups=study_density['study_id']))\n",
    "    \n",
    "    train_studies = study_density.iloc[train_idx]['study_id']\n",
    "    temp_studies = study_density.iloc[temp_idx]['study_id']\n",
    "    \n",
    "    val_ratio_adjusted = val_ratio / (1 - train_ratio)\n",
    "    val_splitter = GroupShuffleSplit(n_splits=1, train_size=val_ratio_adjusted, random_state=random_state)\n",
    "    val_idx, test_idx = next(val_splitter.split(\n",
    "        study_density.iloc[temp_idx], \n",
    "        groups=study_density.iloc[temp_idx]['study_id']\n",
    "    ))\n",
    "    \n",
    "    val_studies = temp_studies.iloc[val_idx]\n",
    "    test_studies = temp_studies.iloc[test_idx]\n",
    "    \n",
    "    train_df = df[df['study_id'].isin(train_studies)]\n",
    "    val_df = df[df['study_id'].isin(val_studies)]\n",
    "    test_df = df[df['study_id'].isin(test_studies)]\n",
    "    \n",
    "    print(\"\\nDataset split statistics:\")\n",
    "    print(f\"Total studies: {len(df['study_id'].unique())}\")\n",
    "    print(f\"Train studies: {len(train_studies)} ({len(train_df)} images)\")\n",
    "    print(f\"Val studies: {len(val_studies)} ({len(val_df)} images)\")\n",
    "    print(f\"Test studies: {len(test_studies)} ({len(test_df)} images)\")\n",
    "    \n",
    "    print(\"\\nDensity distribution in splits:\")\n",
    "    print(\"\\nTrain:\")\n",
    "    print(train_df['breast_density'].value_counts(normalize=True))\n",
    "    print(\"\\nValidation:\")\n",
    "    print(val_df['breast_density'].value_counts(normalize=True))\n",
    "    print(\"\\nTest:\")\n",
    "    print(test_df['breast_density'].value_counts(normalize=True))\n",
    "    \n",
    "    base_path = csv_path.rsplit('.', 1)[0]\n",
    "    train_df.to_csv(f\"{base_path}_train.csv\", index=False)\n",
    "    val_df.to_csv(f\"{base_path}_val.csv\", index=False)\n",
    "    test_df.to_csv(f\"{base_path}_test.csv\", index=False)\n",
    "    \n",
    "    print(\"\\nFiles saved:\")\n",
    "    print(f\"{base_path}_train.csv\")\n",
    "    print(f\"{base_path}_val.csv\")\n",
    "    print(f\"{base_path}_test.csv\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = r'D:\\RESEARCH\\MV-DEFEAT\\datasets\\ThongNhat\\ThongNhat_labels.csv'\n",
    "    train_df, val_df, test_df = split_dataset(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original density distribution:\n",
      "Label\n",
      "C    0.625551\n",
      "B    0.196769\n",
      "D    0.177680\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Dataset split statistics:\n",
      "Total patients: 641\n",
      "Train patients: 384 (404 images)\n",
      "Val patients: 128 (139 images)\n",
      "Test patients: 129 (138 images)\n",
      "\n",
      "Density distribution in splits:\n",
      "\n",
      "Train:\n",
      "Label\n",
      "C    0.633663\n",
      "B    0.198020\n",
      "D    0.168317\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation:\n",
      "Label\n",
      "C    0.625899\n",
      "D    0.187050\n",
      "B    0.187050\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test:\n",
      "Label\n",
      "C    0.601449\n",
      "B    0.202899\n",
      "D    0.195652\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Files saved:\n",
      "D:\\RESEARCH\\MV-DEFEAT\\datasets\\ThongNhat\\ThongNhat_labels_train.csv\n",
      "D:\\RESEARCH\\MV-DEFEAT\\datasets\\ThongNhat\\ThongNhat_labels_val.csv\n",
      "D:\\RESEARCH\\MV-DEFEAT\\datasets\\ThongNhat\\ThongNhat_labels_test.csv\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(csv_path, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2, random_state=42):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(\"Original density distribution:\")\n",
    "    original_dist = df['Label'].value_counts(normalize=True)\n",
    "    print(original_dist)\n",
    "    \n",
    "    train_splitter = GroupShuffleSplit(n_splits=1, train_size=train_ratio, random_state=random_state)\n",
    "    train_idx, temp_idx = next(train_splitter.split(df, groups=df['Patient']))\n",
    "    \n",
    "    train_df = df.iloc[train_idx]\n",
    "    temp_df = df.iloc[temp_idx]\n",
    "    \n",
    "    val_ratio_adjusted = val_ratio / (1 - train_ratio)\n",
    "    val_splitter = GroupShuffleSplit(n_splits=1, train_size=val_ratio_adjusted, random_state=random_state)\n",
    "    val_idx, test_idx = next(val_splitter.split(temp_df, groups=temp_df['Patient']))\n",
    "    \n",
    "    val_df = temp_df.iloc[val_idx]\n",
    "    test_df = temp_df.iloc[test_idx]\n",
    "\n",
    "    print(\"\\nDataset split statistics:\")\n",
    "    print(f\"Total patients: {len(df['Patient'].unique())}\")\n",
    "    print(f\"Train patients: {len(train_df['Patient'].unique())} ({len(train_df)} images)\")\n",
    "    print(f\"Val patients: {len(val_df['Patient'].unique())} ({len(val_df)} images)\")\n",
    "    print(f\"Test patients: {len(test_df['Patient'].unique())} ({len(test_df)} images)\")\n",
    "    \n",
    "    print(\"\\nDensity distribution in splits:\")\n",
    "    print(\"\\nTrain:\")\n",
    "    print(train_df['Label'].value_counts(normalize=True))\n",
    "    print(\"\\nValidation:\")\n",
    "    print(val_df['Label'].value_counts(normalize=True))\n",
    "    print(\"\\nTest:\")\n",
    "    print(test_df['Label'].value_counts(normalize=True))\n",
    "    \n",
    "    base_path = csv_path.rsplit('.', 1)[0]\n",
    "    train_df.to_csv(f\"{base_path}_train.csv\", index=False)\n",
    "    val_df.to_csv(f\"{base_path}_val.csv\", index=False)\n",
    "    test_df.to_csv(f\"{base_path}_test.csv\", index=False)\n",
    "    \n",
    "    print(\"\\nFiles saved:\")\n",
    "    print(f\"{base_path}_train.csv\")\n",
    "    print(f\"{base_path}_val.csv\")\n",
    "    print(f\"{base_path}_test.csv\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = r'D:\\RESEARCH\\MV-DEFEAT\\datasets\\ThongNhat\\ThongNhat_labels.csv'\n",
    "    train_df, val_df, test_df = split_dataset(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weighted sampling distribution:\n",
      "\n",
      "Train distribution:\n",
      "breast_density\n",
      "DENSITY C    0.614500\n",
      "DENSITY D    0.220000\n",
      "DENSITY B    0.157167\n",
      "DENSITY A    0.008333\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation distribution:\n",
      "breast_density\n",
      "DENSITY C    0.9815\n",
      "DENSITY D    0.0135\n",
      "DENSITY B    0.0050\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test distribution:\n",
      "breast_density\n",
      "DENSITY C    0.9980\n",
      "DENSITY D    0.0015\n",
      "DENSITY B    0.0005\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                               study_id                         series_id  \\\n",
       " 16     ac4975eb788af8b7e15cafca9ac9a1c9  bc183447730d58709da1af503d7c469c   \n",
       " 17     ac4975eb788af8b7e15cafca9ac9a1c9  bc183447730d58709da1af503d7c469c   \n",
       " 18     ac4975eb788af8b7e15cafca9ac9a1c9  bc183447730d58709da1af503d7c469c   \n",
       " 19     ac4975eb788af8b7e15cafca9ac9a1c9  bc183447730d58709da1af503d7c469c   \n",
       " 20     87f322198db11b86e20ad96ea29eb010  3e63436aedec442a7b3bafd0158cfce1   \n",
       " ...                                 ...                               ...   \n",
       " 19991  8db1b8ba11d6d804141f1fa4cf91b614  e26615e35922f3b06ca09a742de0ed8b   \n",
       " 19996  b3c8969cd2accfa4dbb2aece1f7158ab  69d7f07ea04572dad5e5aa62fbcfc4b7   \n",
       " 19997  b3c8969cd2accfa4dbb2aece1f7158ab  69d7f07ea04572dad5e5aa62fbcfc4b7   \n",
       " 19998  b3c8969cd2accfa4dbb2aece1f7158ab  69d7f07ea04572dad5e5aa62fbcfc4b7   \n",
       " 19999  b3c8969cd2accfa4dbb2aece1f7158ab  69d7f07ea04572dad5e5aa62fbcfc4b7   \n",
       " \n",
       "                                image_id laterality view_position  height  \\\n",
       " 16     500984f90c9b5ddcf00944b0ecc86260          L            CC    3518   \n",
       " 17     16724df38e04be5e6ae534eda2185885          L           MLO    3518   \n",
       " 18     0b093271f61bac44c7018c935a2ac904          R            CC    3518   \n",
       " 19     da99ea2c75f3021f70607841c0f235b7          R           MLO    3518   \n",
       " 20     8c65a1d8e2429e17ca834ee1866af246          L            CC    3518   \n",
       " ...                                 ...        ...           ...     ...   \n",
       " 19991  bf84bfc8d2bda8d0b2c58b1578e7c694          L           MLO    2812   \n",
       " 19996  4689616c3d0b46fcba7a771107730791          R            CC    3580   \n",
       " 19997  3c22491bcf1d0b004715c28d80981cdd          L            CC    3580   \n",
       " 19998  d443b9725e331b8b27589aa725597801          R           MLO    3580   \n",
       " 19999  45c1239cc36b0e672f0072707fd05c6f          L           MLO    3580   \n",
       " \n",
       "        width breast_birads breast_density     split  \n",
       " 16      2800     BI-RADS 1      DENSITY C  training  \n",
       " 17      2800     BI-RADS 1      DENSITY C  training  \n",
       " 18      2800     BI-RADS 1      DENSITY C  training  \n",
       " 19      2800     BI-RADS 1      DENSITY C  training  \n",
       " 20      2800     BI-RADS 1      DENSITY C  training  \n",
       " ...      ...           ...            ...       ...  \n",
       " 19991   2012     BI-RADS 2      DENSITY C  training  \n",
       " 19996   2702     BI-RADS 2      DENSITY C  training  \n",
       " 19997   2702     BI-RADS 2      DENSITY C  training  \n",
       " 19998   2686     BI-RADS 2      DENSITY C  training  \n",
       " 19999   2670     BI-RADS 2      DENSITY C  training  \n",
       " \n",
       " [12000 rows x 10 columns],\n",
       "                                study_id                         series_id  \\\n",
       " 0      b8d273e8601f348d3664778dae0e7e0b  b36517b9cbbcfd286a7ae04f643af97a   \n",
       " 1      b8d273e8601f348d3664778dae0e7e0b  b36517b9cbbcfd286a7ae04f643af97a   \n",
       " 2      b8d273e8601f348d3664778dae0e7e0b  b36517b9cbbcfd286a7ae04f643af97a   \n",
       " 3      b8d273e8601f348d3664778dae0e7e0b  b36517b9cbbcfd286a7ae04f643af97a   \n",
       " 4      8269f5971eaca3e5d3772d1796e6bd7a  d931832a0815df082c085b6e09d20aac   \n",
       " ...                                 ...                               ...   \n",
       " 19979  90bc5662aa41f8228f1dd2157b9dc6f7  1310924c0c72ad0cb5f060b15dbeabb5   \n",
       " 19984  0bfb4e582c69fa4d06d011e637b44fe8  66a2bae2aa8fdf3b875d2d1dec56eaba   \n",
       " 19985  0bfb4e582c69fa4d06d011e637b44fe8  66a2bae2aa8fdf3b875d2d1dec56eaba   \n",
       " 19986  0bfb4e582c69fa4d06d011e637b44fe8  66a2bae2aa8fdf3b875d2d1dec56eaba   \n",
       " 19987  0bfb4e582c69fa4d06d011e637b44fe8  66a2bae2aa8fdf3b875d2d1dec56eaba   \n",
       " \n",
       "                                image_id laterality view_position  height  \\\n",
       " 0      d8125545210c08e1b1793a5af6458ee2          L            CC    3518   \n",
       " 1      290c658f4e75a3f83ec78a847414297c          L           MLO    3518   \n",
       " 2      cd0fc7bc53ac632a11643ac4cc91002a          R            CC    3518   \n",
       " 3      71638b1e853799f227492bfb08a01491          R           MLO    3518   \n",
       " 4      dd9ce3288c0773e006a294188aadba8e          L            CC    3518   \n",
       " ...                                 ...        ...           ...     ...   \n",
       " 19979  55da29720bda5a9c3e6aea4992c30046          L            CC    2812   \n",
       " 19984  810d2682b243d129465ce043403278e0          R           MLO    2812   \n",
       " 19985  5cc4d08c6d8d91172761b30a2e63a8c0          L           MLO    2812   \n",
       " 19986  11d58b8d2f895c5dd03b55dfb75b391c          R            CC    2812   \n",
       " 19987  1bf6f2c0191b9bb14de79031cfdb7f71          L            CC    2812   \n",
       " \n",
       "        width breast_birads breast_density     split  \n",
       " 0       2800     BI-RADS 2      DENSITY C  training  \n",
       " 1       2800     BI-RADS 2      DENSITY C  training  \n",
       " 2       2800     BI-RADS 2      DENSITY C  training  \n",
       " 3       2800     BI-RADS 2      DENSITY C  training  \n",
       " 4       2800     BI-RADS 1      DENSITY C  training  \n",
       " ...      ...           ...            ...       ...  \n",
       " 19979   2012     BI-RADS 2      DENSITY C  training  \n",
       " 19984   2012     BI-RADS 2      DENSITY D  training  \n",
       " 19985   2012     BI-RADS 1      DENSITY D  training  \n",
       " 19986   2012     BI-RADS 2      DENSITY D  training  \n",
       " 19987   2012     BI-RADS 1      DENSITY D  training  \n",
       " \n",
       " [4000 rows x 10 columns],\n",
       "                                study_id                         series_id  \\\n",
       " 8      fa4dcd0f3ba24e86fc8dc25091f7ebd5  a78f4822d806b4f69ba9f0e0c68778b4   \n",
       " 9      fa4dcd0f3ba24e86fc8dc25091f7ebd5  a78f4822d806b4f69ba9f0e0c68778b4   \n",
       " 10     fa4dcd0f3ba24e86fc8dc25091f7ebd5  a78f4822d806b4f69ba9f0e0c68778b4   \n",
       " 11     fa4dcd0f3ba24e86fc8dc25091f7ebd5  a78f4822d806b4f69ba9f0e0c68778b4   \n",
       " 12     0a0c5108270e814818c1ad002482ce74  ca7f2ada530075dd3cf15df6ee51c835   \n",
       " ...                                 ...                               ...   \n",
       " 19919  8a99da5bd35a36d7b375b45839c7825d  37e6540581ab2b91c94a060882330c4b   \n",
       " 19992  f2093a752e6b44df5990f5fd38c99dd2  2b1b2b8f48abab9819c0b3d091e152ee   \n",
       " 19993  f2093a752e6b44df5990f5fd38c99dd2  2b1b2b8f48abab9819c0b3d091e152ee   \n",
       " 19994  f2093a752e6b44df5990f5fd38c99dd2  2b1b2b8f48abab9819c0b3d091e152ee   \n",
       " 19995  f2093a752e6b44df5990f5fd38c99dd2  2b1b2b8f48abab9819c0b3d091e152ee   \n",
       " \n",
       "                                image_id laterality view_position  height  \\\n",
       " 8      a3d0e2394d7db36afab1b6e5e24da798          L            CC    3518   \n",
       " 9      48b243704d16570155df12995a284b61          L           MLO    3518   \n",
       " 10     bd6af21b792a7efb367407e94678fa1f          R            CC    3518   \n",
       " 11     633ede597d7e514fe672fbdfc8c269eb          R           MLO    3518   \n",
       " 12     1b66d3ea1dae116b7c0e87e3caab3340          L            CC    3518   \n",
       " ...                                 ...        ...           ...     ...   \n",
       " 19919  17fd3cff7b8e9e785fba7deaff40306c          R           MLO    3580   \n",
       " 19992  6d33109be809f7813b5e387387072fdb          R           MLO    2812   \n",
       " 19993  9edcc7df8ac157fb2fdbecb9baae9f2f          L           MLO    2812   \n",
       " 19994  afabc6826bd2b80ae823b94df5e71f29          L            CC    2812   \n",
       " 19995  ea732154d149f619b20070b78060ae65          R            CC    2812   \n",
       " \n",
       "        width breast_birads breast_density     split  \n",
       " 8       2800     BI-RADS 1      DENSITY C  training  \n",
       " 9       2800     BI-RADS 1      DENSITY C  training  \n",
       " 10      2800     BI-RADS 1      DENSITY C  training  \n",
       " 11      2800     BI-RADS 1      DENSITY C  training  \n",
       " 12      2800     BI-RADS 1      DENSITY C  training  \n",
       " ...      ...           ...            ...       ...  \n",
       " 19919   2549     BI-RADS 1      DENSITY C  training  \n",
       " 19992   2012     BI-RADS 2      DENSITY C  training  \n",
       " 19993   2012     BI-RADS 1      DENSITY C  training  \n",
       " 19994   2012     BI-RADS 1      DENSITY C  training  \n",
       " 19995   2012     BI-RADS 2      DENSITY C  training  \n",
       " \n",
       " [4000 rows x 10 columns])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def weighted_split_dataset(csv_path, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split dataset using weighted random sampling to handle class imbalance\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    class_counts = df['breast_density'].value_counts()\n",
    "    class_weights = 1 / class_counts\n",
    "    class_weights = class_weights / class_weights.sum()  \n",
    "    \n",
    "    study_density = df.groupby('study_id')['breast_density'].first().reset_index()\n",
    "    study_weights = study_density['breast_density'].map(class_weights)\n",
    "    \n",
    "    train_studies = study_density['study_id'].sample(\n",
    "        n=int(len(study_density) * train_ratio),\n",
    "        weights=study_weights,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    remaining_studies = study_density[~study_density['study_id'].isin(train_studies)]\n",
    "    remaining_weights = remaining_studies['breast_density'].map(class_weights)\n",
    "    \n",
    "    val_ratio_adjusted = val_ratio / (1 - train_ratio)\n",
    "    val_size = int(len(remaining_studies) * val_ratio_adjusted)\n",
    "    \n",
    "    val_studies = remaining_studies['study_id'].sample(\n",
    "        n=val_size,\n",
    "        weights=remaining_weights,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    test_studies = remaining_studies[~remaining_studies['study_id'].isin(val_studies)]['study_id']\n",
    "    \n",
    "    train_df = df[df['study_id'].isin(train_studies)]\n",
    "    val_df = df[df['study_id'].isin(val_studies)]\n",
    "    test_df = df[df['study_id'].isin(test_studies)]\n",
    "    \n",
    "    print(\"\\nWeighted sampling distribution:\")\n",
    "    print(\"\\nTrain distribution:\")\n",
    "    print(train_df['breast_density'].value_counts(normalize=True))\n",
    "    print(\"\\nValidation distribution:\")\n",
    "    print(val_df['breast_density'].value_counts(normalize=True))\n",
    "    print(\"\\nTest distribution:\")\n",
    "    print(test_df['breast_density'].value_counts(normalize=True))\n",
    "\n",
    "    base_path = csv_path.rsplit('.', 1)[0]\n",
    "    train_df.to_csv(f\"{base_path}_weighted_train.csv\", index=False)\n",
    "    val_df.to_csv(f\"{base_path}_weighted_val.csv\", index=False)\n",
    "    test_df.to_csv(f\"{base_path}_weighted_test.csv\", index=False)\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "weighted_split_dataset(r'D:\\RESEARCH\\MV-DEFEAT\\datasets\\VinDr-mammo\\breast-level_annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def crop_breast_region(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    _, binary = cv2.threshold(gray, 5, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "\n",
    "        cropped = img[y:y+h, x:x+w]\n",
    "        \n",
    "        return cropped\n",
    "    \n",
    "    return None\n",
    "\n",
    "image_path = r\"datasets\\patients_images\\new_data\\VUONG_THI_DUA 13064504\\Left - CC.jpg\"\n",
    "cropped_image = crop_breast_region(image_path)\n",
    "\n",
    "if cropped_image is not None:\n",
    "    cv2.imwrite('cropped_breast.jpg', cropped_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
